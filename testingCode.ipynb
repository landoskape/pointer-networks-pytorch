{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48dce808-de85-4a72-9f1a-bb7d96259ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import train_sort_atl as ts\n",
    "from train_sort_atl import AverageMeter, mainArguments, masked_accuracy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06178276-5740-448e-b084-64b9be685c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import dataset\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4137c203-0af2-4627-a6ca-a88ecd8e99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_seq.shape: torch.Size([256, 10, 100])\n",
      "Embedded.shape: torch.Size([256, 10, 8])\n",
      "encoder_outputs.shape: torch.Size([256, 10, 16])\n",
      "encoder_hidden.shapes: torch.Size([2, 256, 8]) -- torch.Size([2, 256, 8])\n",
      "post bidi - encoder_outputs.shape: torch.Size([256, 10, 8])\n",
      "encoder_h_n.shape: torch.Size([1, 2, 256, 8])\n",
      "encoder_c_n.shape: torch.Size([1, 2, 256, 8])\n",
      "decoder_input.shape: torch.Size([256, 8])\n",
      "decoder_hidden.shape: torch.Size([256, 8]) -- torch.Size([256, 8])\n",
      "input_lengths.shape: torch.Size([256])\n",
      "each_len_tensor.shape: torch.Size([256, 10, 10])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "hi",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     44\u001b[0m log_pointer_score, argmax_pointer, mask \u001b[38;5;241m=\u001b[39m net(seq, length)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m unrolled \u001b[38;5;241m=\u001b[39m log_pointer_score\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, log_pointer_score\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(unrolled, target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: hi"
     ]
    }
   ],
   "source": [
    "userArgs={\n",
    "    'low':0,\n",
    "    'high':100,\n",
    "    'min_length':5,\n",
    "    'max_length':10,\n",
    "    'train_samples':100000,\n",
    "    'test_samples':1000,\n",
    "    'emb_dim':8,\n",
    "    'batch_size':256,\n",
    "    'epochs':100,\n",
    "    'lr':5e-3,\n",
    "    'wd':1e-5,\n",
    "    'workers':4,\n",
    "    'no_cuda':False,\n",
    "    'seed':None\n",
    "}\n",
    "args = mainArguments(**userArgs)\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "cudnn.benchmark = True if use_cuda else False\n",
    "\n",
    "train_set = dataset.IntegerSortDataset(num_samples=args.train_samples, high=args.high, min_len=args.min_length, max_len=args.max_length, seed=1)\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, collate_fn=dataset.sparse_seq_collate_fn)\n",
    "\n",
    "test_set = dataset.IntegerSortDataset(num_samples=args.test_samples, high=args.high, min_len=args.min_length, max_len=args.max_length, seed=2)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, collate_fn=dataset.sparse_seq_collate_fn)\n",
    "\n",
    "net = model.PointerNet(input_dim=args.high, embedding_dim=args.emb_dim, hidden_size=args.emb_dim).to(device)\n",
    "optimizer = Adam(net.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "\n",
    "train_loss = AverageMeter()\n",
    "train_accuracy = AverageMeter()\n",
    "test_loss = AverageMeter()\n",
    "test_accuracy = AverageMeter()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    # Train\n",
    "    net.train()\n",
    "    for batch_idx, (seq, length, target) in enumerate(train_loader):\n",
    "        seq, length, target = seq.to(device), length.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        log_pointer_score, argmax_pointer, mask = net(seq, length)\n",
    "        \n",
    "        raise ValueError('hi')\n",
    "        \n",
    "        unrolled = log_pointer_score.view(-1, log_pointer_score.size(-1))\n",
    "        loss = F.nll_loss(unrolled, target.view(-1), ignore_index=-1)\n",
    "        assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.update(loss.item(), seq.size(0))\n",
    "\n",
    "        mask = mask[:, 0, :]\n",
    "        train_accuracy.update(masked_accuracy(argmax_pointer, target, mask).item(), mask.int().sum().item())\n",
    "\n",
    "        # if batch_idx % 20 == 0:\n",
    "        #     print('Epoch {}: Train [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.6f}'\n",
    "        #           .format(epoch, batch_idx * len(seq), len(train_loader.dataset),\n",
    "        #                   100. * batch_idx / len(train_loader), train_loss.avg, train_accuracy.avg))\n",
    "\n",
    "    # Test\n",
    "    net.eval()\n",
    "    for seq, length, target in test_loader:\n",
    "        seq, length, target = seq.to(device), length.to(device), target.to(device)\n",
    "\n",
    "        log_pointer_score, argmax_pointer, mask = net(seq, length)\n",
    "        unrolled = log_pointer_score.view(-1, log_pointer_score.size(-1))\n",
    "        loss = F.nll_loss(unrolled, target.view(-1), ignore_index=-1)\n",
    "        assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "        test_loss.update(loss.item(), seq.size(0))\n",
    "\n",
    "        mask = mask[:, 0, :]\n",
    "        test_accuracy.update(masked_accuracy(argmax_pointer, target, mask).item(), mask.int().sum().item())\n",
    "    print('Epoch {}: Test\\tLoss: {:.6f}\\tAccuracy: {:.6f}'.format(epoch, test_loss.avg, test_accuracy.avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f497a-16ea-431d-a0ac-5efb1e6cdb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0992b56b-82cf-47e4-b388-a68b10d8a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = IntegerSortDataset(num_samples=args.test_samples, high=args.high, min_len=args.min_length, max_len=args.max_length, seed=2)\n",
    "# test_loader = DataLoader(dataset=test_set, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, collate_fn=sparse_seq_collate_fn)\n",
    "\n",
    "# model = PointerNet(input_dim=args.high, embedding_dim=args.emb_dim, hidden_size=args.emb_dim).to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "\n",
    "# train_loss = AverageMeter()\n",
    "# train_accuracy = AverageMeter()\n",
    "# test_loss = AverageMeter()\n",
    "# test_accuracy = AverageMeter()\n",
    "\n",
    "# for epoch in range(args.epochs):\n",
    "#     # Train\n",
    "#     model.train()\n",
    "#     for batch_idx, (seq, length, target) in enumerate(train_loader):\n",
    "#         seq, length, target = seq.to(device), length.to(device), target.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         log_pointer_score, argmax_pointer, mask = model(seq, length)\n",
    "\n",
    "#         unrolled = log_pointer_score.view(-1, log_pointer_score.size(-1))\n",
    "#         loss = F.nll_loss(unrolled, target.view(-1), ignore_index=-1)\n",
    "#         assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss.update(loss.item(), seq.size(0))\n",
    "\n",
    "#         mask = mask[:, 0, :]\n",
    "#         train_accuracy.update(masked_accuracy(argmax_pointer, target, mask).item(), mask.int().sum().item())\n",
    "\n",
    "#         # if batch_idx % 20 == 0:\n",
    "#         #     print('Epoch {}: Train [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.6f}'\n",
    "#         #           .format(epoch, batch_idx * len(seq), len(train_loader.dataset),\n",
    "#         #                   100. * batch_idx / len(train_loader), train_loss.avg, train_accuracy.avg))\n",
    "\n",
    "#     # Test\n",
    "#     model.eval()\n",
    "#     for seq, length, target in test_loader:\n",
    "#         seq, length, target = seq.to(device), length.to(device), target.to(device)\n",
    "\n",
    "#         log_pointer_score, argmax_pointer, mask = model(seq, length)\n",
    "#         unrolled = log_pointer_score.view(-1, log_pointer_score.size(-1))\n",
    "#         loss = F.nll_loss(unrolled, target.view(-1), ignore_index=-1)\n",
    "#         assert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n",
    "\n",
    "#         test_loss.update(loss.item(), seq.size(0))\n",
    "\n",
    "#         mask = mask[:, 0, :]\n",
    "#         test_accuracy.update(masked_accuracy(argmax_pointer, target, mask).item(), mask.int().sum().item())\n",
    "#     print('Epoch {}: Test\\tLoss: {:.6f}\\tAccuracy: {:.6f}'.format(epoch, test_loss.avg, test_accuracy.avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3c88a-7b4e-4ad2-98ba-792eb74ff6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146dbbc-979d-4288-bfdf-f969ea039cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5737f24-5ffe-40cf-b409-ac35539edfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324bcc0-daab-4db4-912a-7a6c778cc7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720d431-7b34-4881-8060-8be0d0c6abc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7527279-d40f-4945-98e7-24b6a66eb57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
